# -*- coding: utf-8 -*-
"""Googlenet.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uTabi4-pCIP0Rf4XhjnfN5A0Nk4pqaSb
"""





import numpy as np
import matplotlib.pyplot as plt
import keras
from keras.layers import *
from keras.models import *
from keras_preprocessing import image
from os import listdir
from os.path import isfile, join
import numpy as np
import pandas as pd
import os
import os
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
from tensorflow.keras import regularizers
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Input, Flatten, Dense, Dropout, BatchNormalization
from tensorflow.keras.layers import Conv2D, MaxPooling2D, AveragePooling2D, ZeroPadding2D
from tensorflow.keras.layers import Concatenate
from tensorflow.keras.preprocessing.image import image
from tensorflow.keras.optimizers import Adam, SGD
from sklearn.model_selection import KFold, StratifiedKFold
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from imblearn.over_sampling import SMOTE
from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score
from PIL import Image
from matplotlib import pyplot as plt

train_data = pd.read_csv('train.csv',dtype=str)
validation_data=pd.read_csv('test.csv',dtype=str)
Y = train_data[['label']]
X_train=train_data[['train_images']]
train_labels=list(train_data['label'])
kf = KFold(n_splits = 5)
Y_val=validation_data['label']

idg = ImageDataGenerator(width_shift_range=0.1,
                         height_shift_range=0.1,
                         zoom_range=0.3,
                         fill_mode='nearest',
                         horizontal_flip = True,
                         rescale=1./255)

def inception(x, filters):
    # 1x1
    path1 = Conv2D(filters=filters[0], kernel_size=(1,1), strides=1, padding='same', activation='relu')(x)

    # 1x1->3x3
    path2 = Conv2D(filters=filters[1][0], kernel_size=(1,1), strides=1, padding='same', activation='relu')(x)
    path2 = Conv2D(filters=filters[1][1], kernel_size=(3,3), strides=1, padding='same', activation='relu')(path2)
    
    # 1x1->5x5
    path3 = Conv2D(filters=filters[2][0], kernel_size=(1,1), strides=1, padding='same', activation='relu')(x)
    path3 = Conv2D(filters=filters[2][1], kernel_size=(5,5), strides=1, padding='same', activation='relu')(path3)

    # 3x3->1x1
    path4 = MaxPooling2D(pool_size=(3,3), strides=1, padding='same')(x)
    path4 = Conv2D(filters=filters[3], kernel_size=(1,1), strides=1, padding='same', activation='relu')(path4)

    return Concatenate(axis=-1)([path1,path2,path3,path4])


def auxiliary(x, name=None):
    layer = AveragePooling2D(pool_size=(5,5), strides=3, padding='valid')(x)
    layer = Conv2D(filters=128, kernel_size=(1,1), strides=1, padding='same', activation='relu')(layer)
    layer = Flatten()(layer)
    layer = Dense(units=256, activation='relu')(layer)
    layer = Dropout(0.4)(layer)
    layer = Dense(units=CLASS_NUM, activation='softmax', name=name)(layer)
    return layer


def googlenet():
    layer_in = Input(shape=IMAGE_SHAPE)
    
    # stage-1
    layer = Conv2D(filters=64, kernel_size=(7,7), strides=2, padding='same', activation='relu')(layer_in)
    layer = MaxPooling2D(pool_size=(3,3), strides=2, padding='same')(layer)
    layer = BatchNormalization()(layer)

    # stage-2
    layer = Conv2D(filters=64, kernel_size=(1,1), strides=1, padding='same', activation='relu')(layer)
    layer = Conv2D(filters=192, kernel_size=(3,3), strides=1, padding='same', activation='relu')(layer)
    layer = BatchNormalization()(layer)
    layer = MaxPooling2D(pool_size=(3,3), strides=2, padding='same')(layer)

    # stage-3
    layer = inception(layer, [ 64,  (96,128), (16,32), 32]) #3a
    layer = inception(layer, [128, (128,192), (32,96), 64]) #3b
    layer = MaxPooling2D(pool_size=(3,3), strides=2, padding='same')(layer)
    
    # stage-4
    layer = inception(layer, [192,  (96,208),  (16,48),  64]) #4a
    aux1  = auxiliary(layer, name='aux1')
    layer = inception(layer, [160, (112,224),  (24,64),  64]) #4b
    layer = inception(layer, [128, (128,256),  (24,64),  64]) #4c
    layer = inception(layer, [112, (144,288),  (32,64),  64]) #4d
    aux2  = auxiliary(layer, name='aux2')
    layer = inception(layer, [256, (160,320), (32,128), 128]) #4e
    layer = MaxPooling2D(pool_size=(3,3), strides=2, padding='same')(layer)
    
    # stage-5
    layer = inception(layer, [256, (160,320), (32,128), 128]) #5a
    layer = inception(layer, [384, (192,384), (48,128), 128]) #5b
    layer = AveragePooling2D(pool_size=(7,7), strides=1, padding='valid')(layer)
    
    # stage-6
    layer = Flatten()(layer)
    layer = Dropout(0.4)(layer)
    layer = Dense(units=256, activation='linear')(layer)
    main = Dense(units=CLASS_NUM, activation='softmax', name='main')(layer)
    
    model = Model(inputs=layer_in, outputs=[main, aux1, aux2])
    
    return model

VALIDATION_ACCURACY = []
accuracy_google_smote=[]
f1_score_google_smote=[]
sensitivity_google_smote=[]
precision_google_smote=[]
precision=[]
VALIDAITON_LOSS = []
f1_score=[]
recall=[]
image_dir_train='/content/CovidDataset/Train'
image_dir_val='/content/CovidDataset/Val'
save_dir = '/saved_models/'
fold_var = 1

for train_index, val_index in kf.split(np.zeros(224),Y):
    
    train_data_generator = idg.flow_from_dataframe(train_data, directory = image_dir_train,
                    x_col = "train_images", y_col = "label", target_size=(224, 224),
                    class_mode = "binary", shuffle = True)
    valid_data_generator  = idg.flow_from_dataframe(validation_data, directory = image_dir_val,
                x_col = "val_images", y_col = "label", target_size=(224, 224),
                class_mode = "binary", shuffle = True)
    sm = SMOTE(random_state=2)
    x_train,y_train= sm.fit_resample(train_data_generator,Y)

    
    # CREATE NEW MODEL
    model=googlenet()
    model.compile(loss='mse', optimizer='adam')
    history = model.fit(x_train,y_train,
            epochs=100,           
            validation_data=valid_data_generator)
    

    VALIDATION_ACCURACY.append(results['accuracy'])
    VALIDATION_LOSS.append(results['loss'])
    accuracy_google_smote.append(results['accuracy'])
    tf.keras.backend.clear_session()
    test_y=[]
    train_y=[]
    for i in os.listdir("CovidDataset/Val/Normal/"):
      img=image.load_img("CovidDataset/Val/Normal/"+i)
      img=img.resize((224,224), Image.ANTIALIAS)
      img=image.img_to_array(img)
      img=np.expand_dims(img, axis=0)
      p=model.predict_classes(img)
      test_y.append(p[0,0])
      train_y.append(1)
    for i in os.listdir("CovidDataset/Val/Covid/"):
      img=image.load_img("CovidDataset/Val/Covid/"+i)
      img=img.resize((224,224), Image.ANTIALIAS)
      img=image.img_to_array(img)
      img=np.expand_dims(img, axis=0)
      p=model.predict_classes(img)
      test_y.append(p[0,0])
      train_y.append(0)
    train_y=np.array(train_y)
    test_y=np.array(test_y)
    f1_score=f1_score(train_y,test_y)
    f1_score_google_smote.append(f1_score)
    recall=recall_score(train_y,test_y)
    sensitivity_google_smote.append(recall)
    precision=precision_score(train_y,test_y)
    precision_google_smote.append(precision)
    fold_var += 1

acc_avg_google_smote=np.mean(accuracy_google_smote)
acc_std_google_smote=np.std(accuracy_google_smote)
prec_avg_google_smote=np.mean(precision_google_smote)
prec_std_google_smote=np.std(precision_google_smote)
sens_avg_google_smote=np.mean(sensitivity_google_smote)
sens_std_google_smote=np.std(sensitivity_google_smote)
f1_avg_google_smote=np.mean(f1_score_google_smote)
f1_acc_google_smote=np.std(f1_score_google_smote)



VALIDATION_ACCURACY = []
accuracy_google_normal=[]
f1_score_google_normal=[]
sensitivity_google_normal=[]
precision_google_normal=[]
precision=[]
VALIDAITON_LOSS = []
f1_score=[]
recall=[]
image_dir_train='/content/CovidDataset/Train'
image_dir_val='/content/CovidDataset/Val'
save_dir = '/saved_models/'
fold_var = 1

for train_index, val_index in kf.split(np.zeros(224),Y):
    
    train_data_generator = idg.flow_from_dataframe(train_data, directory = image_dir_train,
                    x_col = "train_images", y_col = "label", target_size=(224, 224),
                    class_mode = "binary", shuffle = True)
    valid_data_generator  = idg.flow_from_dataframe(validation_data, directory = image_dir_val,
                x_col = "val_images", y_col = "label", target_size=(224, 224),
                class_mode = "binary", shuffle = True)

    # CREATE NEW MODEL

    model=googlenet()
    model.compile(loss='mse', optimizer='adam')
    history = model.fit(train_data_generator,
            epochs=100,           
            validation_data=valid_data_generator)
 
    VALIDATION_ACCURACY.append(results['accuracy'])
    VALIDATION_LOSS.append(results['loss'])
    accuracy_google_normal.append(results['accuracy'])
    tf.keras.backend.clear_session()
    test_y=[]
    train_y=[]
    for i in os.listdir("CovidDataset/Val/Normal/"):
      img=image.load_img("CovidDataset/Val/Normal/"+i)
      img=img.resize((224,224), Image.ANTIALIAS)
      img=image.img_to_array(img)
      img=np.expand_dims(img, axis=0)
      p=model.predict_classes(img)
      test_y.append(p[0,0])
      train_y.append(1)
    for i in os.listdir("CovidDataset/Val/Covid/"):
      img=image.load_img("CovidDataset/Val/Covid/"+i)
      img=img.resize((224,224), Image.ANTIALIAS)
      img=image.img_to_array(img)
      img=np.expand_dims(img, axis=0)
      p=model.predict_classes(img)
      test_y.append(p[0,0])
      train_y.append(0)
    train_y=np.array(train_y)
    test_y=np.array(test_y)
    f1_score=f1_score(train_y,test_y)
    f1_score_google_normal.append(f1_score)
    recall=recall_score(train_y,test_y)
    sensitivity_google_normal.append(recall)
    precision=precision_score(train_y,test_y)
    precision_google_normal.append(precision)
    fold_var += 1

acc_avg_google_normal=np.mean(accuracy_google_normal)
acc_std_google_normal=np.std(accuracy_google_normal)
prec_avg_google_normal=np.mean(precision_google_normal)
prec_std_google_normal=np.std(precision_google_normal)
sens_avg_google_normal=np.mean(sensitivity_google_normal)
sens_std_google_normal=np.std(sensitivity_google_normal)
f1_avg_google_normal=np.mean(f1_score_google_normal)
f1_acc_google_normal=np.std(f1_score_google_normal)



































